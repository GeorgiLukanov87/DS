{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f119a70-eca5-40ea-a858-83c29f075489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, regexp_replace, sha2, concat_ws\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babacdc0-eff2-4dee-8de4-bb998bc0bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"C:/Users/User/AppData/Local/Programs/Eclipse Adoptium/jdk-11.0.25.9-hotspot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fb57e2-505b-4429-92a0-caf4b39b91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"ETLPipeline\") \\\n",
    "    .setMaster(\"local\") \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"C:/jars/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361e2f14-adbe-46f3-b58f-35a846d0e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d46c72d-7933-4ab1-8fee-1a78ee94b56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Goto:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETLPipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29a11bfd6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7c9fb7-b129-456e-bef9-c6483382c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True),\n",
    "    StructField(\"birth_date\", DateType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "raw_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"customers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ebc9504-42db-4f8c-9987-626bcacdbac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+------------------+-------------+----------+--------+\n",
      "|  id|first_name|last_name|             email| phone_number|birth_date| country|\n",
      "+----+----------+---------+------------------+-------------+----------+--------+\n",
      "|0001|      Jane|  Ivanova| user1@example.com|+359872837997|1983-10-20| Germany|\n",
      "|0002|      Jane|  Ivanova| user2@example.com|+359873576092|1985-03-09|     USA|\n",
      "|0003|      Anna|      Doe| user3@example.com|+359875550487|1983-04-22|     USA|\n",
      "|0004|      John|   Koleva| user4@example.com|+359870819793|1999-02-23| Belgium|\n",
      "|0005|      Ivan|   Petrov| user5@example.com|+359871869307|1991-07-04| Germany|\n",
      "|0006|      John| Georgiev| user6@example.com|+359876716412|1983-05-26| Belgium|\n",
      "|0007|      Ivan|   Petrov| user7@example.com|+359879017692|1983-07-14| Germany|\n",
      "|0008|     Maria|    Smith| user8@example.com|+359874956900|1999-06-08| Ireland|\n",
      "|0009|      Jane|   Koleva| user9@example.com|+359876360207|1990-09-03| Belgium|\n",
      "|0010|      Ivan|    Smith|user10@example.com|+359878083033|1990-12-27| Germany|\n",
      "|0011|      Jane|  Ivanova|user11@example.com|+359872295708|1983-07-08| Ireland|\n",
      "|0012|      Anna|   Koleva|user12@example.com|+359875457368|1997-05-23|     USA|\n",
      "|0013|      Jane|   Petrov|user13@example.com|+359873021631|1990-08-24|     USA|\n",
      "|0014|      John| Georgiev|user14@example.com|+359872243517|1998-08-16|     USA|\n",
      "|0015|      Ivan|   Petrov|user15@example.com|+359875281070|1989-02-15| Germany|\n",
      "|0016|      Jane|  Ivanova|user16@example.com|+359878843570|1994-10-22| Germany|\n",
      "|0017|    Georgi|    Smith|user17@example.com|+359872538086|1975-10-10| Germany|\n",
      "|0018|     Maria|      Doe|user18@example.com|+359872724785|1981-08-13| Germany|\n",
      "|0019|      John|   Koleva|user19@example.com|+359873637168|1984-05-10|Bulgaria|\n",
      "|0020|      Jane|  Ivanova|user20@example.com|+359874159387|1972-06-16| Belgium|\n",
      "+----+----------+---------+------------------+-------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0886041-aa7a-4e51-9deb-b374dd363510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GDPR anonymization - хеширане на лични данни\n",
    "anonymized_df = raw_df.withColumn(\"email_hash\", sha2(col(\"email\"), 256)) \\\n",
    "                      .withColumn(\"phone_hash\", sha2(col(\"phone_number\"), 256)) \\\n",
    "                      .drop(\"email\", \"phone_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d8b6cb-9b47-444b-82e0-56fd89caee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+----------+--------+--------------------+--------------------+\n",
      "|  id|first_name|last_name|birth_date| country|          email_hash|          phone_hash|\n",
      "+----+----------+---------+----------+--------+--------------------+--------------------+\n",
      "|0001|      Jane|  Ivanova|1983-10-20| Germany|b36a83701f1c3191e...|a325ef9b3e751a3ed...|\n",
      "|0002|      Jane|  Ivanova|1985-03-09|     USA|2b3b2b9ce842ab8b6...|f503ab29434ee000e...|\n",
      "|0003|      Anna|      Doe|1983-04-22|     USA|898628e28890f937b...|f80350dfd9b2e2e19...|\n",
      "|0004|      John|   Koleva|1999-02-23| Belgium|40d71d3f998c168e7...|e3fb8f506b7abfe4c...|\n",
      "|0005|      Ivan|   Petrov|1991-07-04| Germany|4d8f4dd97e0c7b6fe...|659cbbe71279ae9a8...|\n",
      "|0006|      John| Georgiev|1983-05-26| Belgium|b430419a8a3fa1ce5...|dae098253612e288f...|\n",
      "|0007|      Ivan|   Petrov|1983-07-14| Germany|38121022af9b425b5...|dcdacfab6a9562d90...|\n",
      "|0008|     Maria|    Smith|1999-06-08| Ireland|675657c179a97bde8...|7f63b2656ffc159d6...|\n",
      "|0009|      Jane|   Koleva|1990-09-03| Belgium|b1e700bec7b4c7c38...|2e26f42d072b80b3e...|\n",
      "|0010|      Ivan|    Smith|1990-12-27| Germany|1cc95683bbb5c4811...|9064f3c7e930708a7...|\n",
      "|0011|      Jane|  Ivanova|1983-07-08| Ireland|69e6267c53626874a...|49322bd766fdbd251...|\n",
      "|0012|      Anna|   Koleva|1997-05-23|     USA|882ecc75a8c8ab735...|1a4c212de70d7c787...|\n",
      "|0013|      Jane|   Petrov|1990-08-24|     USA|f221e7d82b835de7b...|83a067c80413d7af9...|\n",
      "|0014|      John| Georgiev|1998-08-16|     USA|241b6d9462fcc1986...|9862606d4df70a6ad...|\n",
      "|0015|      Ivan|   Petrov|1989-02-15| Germany|19e2f1dbc9e6c95e4...|a5b9b3d7135b253f4...|\n",
      "|0016|      Jane|  Ivanova|1994-10-22| Germany|67df3e41329159dd3...|a1c80197293d54fbd...|\n",
      "|0017|    Georgi|    Smith|1975-10-10| Germany|93aa53ab9b59fb422...|f880e3515a15b618f...|\n",
      "|0018|     Maria|      Doe|1981-08-13| Germany|488500947eaecc908...|d7f078bc63bf3b3f7...|\n",
      "|0019|      John|   Koleva|1984-05-10|Bulgaria|66539894c8b69c8b6...|2a1efb796f5d495e8...|\n",
      "|0020|      Jane|  Ivanova|1972-06-16| Belgium|874c06ecc187c90d2...|82f87c132467a4cb6...|\n",
      "+----+----------+---------+----------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anonymized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "814535e5-1534-417e-a8be-bea97f7938c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Трансформации и нови колони\n",
    "transformed_df = anonymized_df.withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "                              .drop(\"first_name\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5224d889-42b7-4d2e-b830-e0cd082bfbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------------------+--------------------+-------------+\n",
      "|  id|birth_date| country|          email_hash|          phone_hash|    full_name|\n",
      "+----+----------+--------+--------------------+--------------------+-------------+\n",
      "|0001|1983-10-20| Germany|b36a83701f1c3191e...|a325ef9b3e751a3ed...| Jane Ivanova|\n",
      "|0002|1985-03-09|     USA|2b3b2b9ce842ab8b6...|f503ab29434ee000e...| Jane Ivanova|\n",
      "|0003|1983-04-22|     USA|898628e28890f937b...|f80350dfd9b2e2e19...|     Anna Doe|\n",
      "|0004|1999-02-23| Belgium|40d71d3f998c168e7...|e3fb8f506b7abfe4c...|  John Koleva|\n",
      "|0005|1991-07-04| Germany|4d8f4dd97e0c7b6fe...|659cbbe71279ae9a8...|  Ivan Petrov|\n",
      "|0006|1983-05-26| Belgium|b430419a8a3fa1ce5...|dae098253612e288f...|John Georgiev|\n",
      "|0007|1983-07-14| Germany|38121022af9b425b5...|dcdacfab6a9562d90...|  Ivan Petrov|\n",
      "|0008|1999-06-08| Ireland|675657c179a97bde8...|7f63b2656ffc159d6...|  Maria Smith|\n",
      "|0009|1990-09-03| Belgium|b1e700bec7b4c7c38...|2e26f42d072b80b3e...|  Jane Koleva|\n",
      "|0010|1990-12-27| Germany|1cc95683bbb5c4811...|9064f3c7e930708a7...|   Ivan Smith|\n",
      "|0011|1983-07-08| Ireland|69e6267c53626874a...|49322bd766fdbd251...| Jane Ivanova|\n",
      "|0012|1997-05-23|     USA|882ecc75a8c8ab735...|1a4c212de70d7c787...|  Anna Koleva|\n",
      "|0013|1990-08-24|     USA|f221e7d82b835de7b...|83a067c80413d7af9...|  Jane Petrov|\n",
      "|0014|1998-08-16|     USA|241b6d9462fcc1986...|9862606d4df70a6ad...|John Georgiev|\n",
      "|0015|1989-02-15| Germany|19e2f1dbc9e6c95e4...|a5b9b3d7135b253f4...|  Ivan Petrov|\n",
      "|0016|1994-10-22| Germany|67df3e41329159dd3...|a1c80197293d54fbd...| Jane Ivanova|\n",
      "|0017|1975-10-10| Germany|93aa53ab9b59fb422...|f880e3515a15b618f...| Georgi Smith|\n",
      "|0018|1981-08-13| Germany|488500947eaecc908...|d7f078bc63bf3b3f7...|    Maria Doe|\n",
      "|0019|1984-05-10|Bulgaria|66539894c8b69c8b6...|2a1efb796f5d495e8...|  John Koleva|\n",
      "|0020|1972-06-16| Belgium|874c06ecc187c90d2...|82f87c132467a4cb6...| Jane Ivanova|\n",
      "+----+----------+--------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deb2c9a9-08d4-49c6-a433-bcf3d25dec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Филтриране на данни (примерно за EU страни само)\n",
    "filtered_df = transformed_df.filter(col(\"country\").isin(\"Belgium\", \"Bulgaria\", \"Ireland\", \"Germany\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe185aa-76c8-4d03-b6fd-47f9f4cf4ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+--------------------+--------------------+-------------+\n",
      "|  id|birth_date| country|          email_hash|          phone_hash|    full_name|\n",
      "+----+----------+--------+--------------------+--------------------+-------------+\n",
      "|0001|1983-10-20| Germany|b36a83701f1c3191e...|a325ef9b3e751a3ed...| Jane Ivanova|\n",
      "|0004|1999-02-23| Belgium|40d71d3f998c168e7...|e3fb8f506b7abfe4c...|  John Koleva|\n",
      "|0005|1991-07-04| Germany|4d8f4dd97e0c7b6fe...|659cbbe71279ae9a8...|  Ivan Petrov|\n",
      "|0006|1983-05-26| Belgium|b430419a8a3fa1ce5...|dae098253612e288f...|John Georgiev|\n",
      "|0007|1983-07-14| Germany|38121022af9b425b5...|dcdacfab6a9562d90...|  Ivan Petrov|\n",
      "|0008|1999-06-08| Ireland|675657c179a97bde8...|7f63b2656ffc159d6...|  Maria Smith|\n",
      "|0009|1990-09-03| Belgium|b1e700bec7b4c7c38...|2e26f42d072b80b3e...|  Jane Koleva|\n",
      "|0010|1990-12-27| Germany|1cc95683bbb5c4811...|9064f3c7e930708a7...|   Ivan Smith|\n",
      "|0011|1983-07-08| Ireland|69e6267c53626874a...|49322bd766fdbd251...| Jane Ivanova|\n",
      "|0015|1989-02-15| Germany|19e2f1dbc9e6c95e4...|a5b9b3d7135b253f4...|  Ivan Petrov|\n",
      "|0016|1994-10-22| Germany|67df3e41329159dd3...|a1c80197293d54fbd...| Jane Ivanova|\n",
      "|0017|1975-10-10| Germany|93aa53ab9b59fb422...|f880e3515a15b618f...| Georgi Smith|\n",
      "|0018|1981-08-13| Germany|488500947eaecc908...|d7f078bc63bf3b3f7...|    Maria Doe|\n",
      "|0019|1984-05-10|Bulgaria|66539894c8b69c8b6...|2a1efb796f5d495e8...|  John Koleva|\n",
      "|0020|1972-06-16| Belgium|874c06ecc187c90d2...|82f87c132467a4cb6...| Jane Ivanova|\n",
      "+----+----------+--------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35337ea4-ecf5-4c71-921d-d9f9b70170ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o190.json.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:784)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfiltered_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/User/Desktop/jupyter-etl-spark/customers/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1658\u001b[39m, in \u001b[36mDataFrameWriter.json\u001b[39m\u001b[34m(self, path, mode, compression, dateFormat, timestampFormat, lineSep, encoding, ignoreNullFields)\u001b[39m\n\u001b[32m   1649\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   1650\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   1651\u001b[39m     compression=compression,\n\u001b[32m   1652\u001b[39m     dateFormat=dateFormat,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1656\u001b[39m     ignoreNullFields=ignoreNullFields,\n\u001b[32m   1657\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o190.json.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:784)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:377)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "filtered_df.write.mode(\"overwrite\").json(\"C:/Users/User/Desktop/jupyter-etl-spark/customers/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0374b98-1ad7-4c8e-8d03-943028add43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68f1dd-37c6-4238-93bd-0d10c2af7f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
